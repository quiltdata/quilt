import functools
import hashlib
import os
import random
import urllib.parse

import boto3
import botocore
import elasticsearch.helpers
import orjson
from aws_requests_auth.aws_auth import AWSRequestsAuth
from elasticsearch import Elasticsearch

PACKAGE_INDEX_SUFFIX = "_packages"
# we need to filter out GetObject and HeadObject calls generated by the present
#  lambda in order to display accurate analytics in the Quilt catalog
#  a custom user agent enables said filtration
USER_AGENT_EXTRA = " quilt3-lambdas-es-indexer"
ELASTIC_TIMEOUT = 30


class Batcher:
    # json_encode = JSONEncoder(ensure_ascii=False, separators=(",", ":")).encode
    BATCH_INDEXER_BUCKET = os.getenv("ES_INGEST_BUCKET")
    BATCH_MAX_BYTES = int(os.getenv("BATCH_MAX_BYTES", 8_000_000))
    BATCH_MAX_DOCS = int(os.getenv("BATCH_MAX_DOCS", 10_000))

    @staticmethod
    def _make_key():
        return f"{random.randbytes(4).hex()}/object"

    def _reset(self):
        """reset the current batch"""
        self.current_batch: list[bytes] = []
        self.current_batch_size = 0

    def __init__(self, s3client, logger) -> None:
        self.s3_client = s3client
        self.logger = logger
        self._reset()

    def _send_batch(self):
        """send the current batch to S3"""
        if not self.current_batch:
            return
        batch = self.current_batch
        self._reset()
        key = self._make_key()
        self.s3_client.put_object(
            Bucket=self.BATCH_INDEXER_BUCKET,
            Key=key,
            Body=b"".join(batch),
            ContentType="application/json",
        )
        self.logger.debug("Batch sent to s3://%s/%s", self.BATCH_INDEXER_BUCKET, key)

    def append(self, doc: dict):
        # get doc ownership
        doc["_type"] = "_doc"  # ES 6.x compatibility
        data = b"".join(
            map(
                functools.partial(orjson.dumps, option=orjson.OPT_APPEND_NEWLINE),
                filter(None.__ne__, elasticsearch.helpers.expand_action(doc)),
            )
        )
        assert (
            len(data) < self.BATCH_MAX_BYTES
        ), f"Document size {len(data)} exceeds max batch size {self.BATCH_MAX_BYTES}"

        if (
            len(self.current_batch) >= self.BATCH_MAX_DOCS
            or self.current_batch_size + len(data) > self.BATCH_MAX_BYTES
        ):
            self._send_batch()

        self.current_batch.append(data)
        self.current_batch_size += len(data)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """Send the remaining batch on exit"""
        self._send_batch()


def make_s3_client():
    """make a client with a custom user agent string so that we can
    filter the present lambda's requests to S3 from object analytics"""
    configuration = botocore.config.Config(user_agent_extra=USER_AGENT_EXTRA)
    return boto3.client("s3", config=configuration)


def make_elastic(es_endpoint: str, **kwargs) -> Elasticsearch:
    session = boto3.session.Session()
    credentials = session.get_credentials().get_frozen_credentials()
    awsauth = AWSRequestsAuth(
        # These environment variables are automatically set by Lambda
        aws_access_key=credentials.access_key,
        aws_secret_access_key=credentials.secret_key,
        aws_token=credentials.token,
        aws_host=urllib.parse.urlparse(es_endpoint).hostname,
        aws_region=session.region_name,
        aws_service="es",
    )

    return elasticsearch.Elasticsearch(
        hosts=es_endpoint,
        http_auth=awsauth,
        verify_certs=True,
        connection_class=elasticsearch.RequestsHttpConnection,
        **kwargs,
    )


def get_es_aliases(es) -> frozenset[str]:
    return frozenset(a for v in es.indices.get_alias().values() for a in v["aliases"])


def get_object_doc_id(key: str, version_id: str) -> str:
    """
    Generate unique value for every object in the bucket to be used as
    document `_id`. This value must not exceed 512 bytes in size:
    https://www.elastic.co/guide/en/elasticsearch/reference/7.10/mapping-id-field.html.
    # TODO: both object key and version ID are up to 1024 bytes long, so
    # we need to use something like `_hash(key) + _hash(version_id)` to
    # overcome the mentioned size restriction.
    """
    return f"{key}:{version_id}"


def get_manifest_doc_id(manifest_hash: str) -> str:
    assert len(manifest_hash) <= 512, "Manifest hash must not exceed 512 characters"
    return f"mnfst:{manifest_hash}"


def get_manifest_entry_doc_id(manifest_hash: str, logical_key: str) -> str:
    _id = f"entry:{manifest_hash}:{_hash_string(logical_key)}"
    assert len(_id) <= 512, "Manifest entry document ID must not exceed 512 characters"
    return _id


def get_ptr_doc_id(pkg_name: str, tag: str) -> str:
    _id = f"ptr:{_hash_string(pkg_name + '/' + tag)}"
    assert len(_id) <= 512, "Pointer document ID must not exceed 512 characters"
    return _id


def _hash_string(s: str) -> str:
    return hashlib.sha256(s.encode()).hexdigest()
