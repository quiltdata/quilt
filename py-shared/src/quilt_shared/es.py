import functools
import hashlib
import os
import random
import urllib.parse

import boto3
import botocore
import elasticsearch.helpers
import orjson
from aws_requests_auth.aws_auth import AWSRequestsAuth
from elasticsearch import Elasticsearch

PACKAGE_INDEX_SUFFIX = "_packages"
# we need to filter out GetObject and HeadObject calls generated by the present
#  lambda in order to display accurate analytics in the Quilt catalog
#  a custom user agent enables said filtration
USER_AGENT_EXTRA = " quilt3-lambdas-es-indexer"


class Batcher:
    @staticmethod
    def _make_key():
        return f"{random.randbytes(4).hex()}/object"

    def _reset(self):
        """reset the current batch"""
        self.current_batch: list[bytes] = []
        self.current_batch_size = 0

    def __init__(self, s3client, logger, *, bucket: str, batch_max_bytes: int, batch_max_docs: int) -> None:
        if not bucket:
            raise ValueError("bucket must be a non-empty string")
        if batch_max_bytes <= 0 or batch_max_docs <= 0:
            raise ValueError("batch_max_bytes and batch_max_docs must be positive integers")

        self.s3_client = s3client
        self.logger = logger
        self.bucket = bucket
        self.batch_max_bytes = batch_max_bytes
        self.batch_max_docs = batch_max_docs
        self._reset()

    @classmethod
    def from_env(cls, s3client, logger):
        bucket = os.getenv("ES_INGEST_BUCKET")
        if not bucket:
            raise ValueError("ES_INGEST_BUCKET environment variable is not set")
        batch_max_bytes = int(os.getenv("BATCH_MAX_BYTES", 8_000_000))
        batch_max_docs = int(os.getenv("BATCH_MAX_DOCS", 10_000))
        return cls(
            s3client=s3client,
            logger=logger,
            bucket=bucket,
            batch_max_bytes=batch_max_bytes,
            batch_max_docs=batch_max_docs,
        )

    def _send_batch(self):
        """send the current batch to S3"""
        if not self.current_batch:
            return
        batch = self.current_batch
        self._reset()
        key = self._make_key()
        self.s3_client.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=b"".join(batch),
            ContentType="application/json",
        )
        self.logger.debug("Batch sent to s3://%s/%s", self.bucket, key)

    def append(self, doc: dict):
        # get doc ownership
        doc["_type"] = "_doc"  # ES 6.x compatibility
        data = b"".join(
            map(
                functools.partial(orjson.dumps, option=orjson.OPT_APPEND_NEWLINE),
                filter(None.__ne__, elasticsearch.helpers.expand_action(doc)),
            )
        )
        assert (
            len(data) < self.batch_max_bytes
        ), f"Document size {len(data)} exceeds max batch size {self.batch_max_bytes}"

        if (
            len(self.current_batch) >= self.batch_max_docs
            or self.current_batch_size + len(data) > self.batch_max_bytes
        ):
            self._send_batch()

        self.current_batch.append(data)
        self.current_batch_size += len(data)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """Send the remaining batch on exit"""
        self._send_batch()


def make_s3_client():
    """make a client with a custom user agent string so that we can
    filter the present lambda's requests to S3 from object analytics"""
    configuration = botocore.config.Config(user_agent_extra=USER_AGENT_EXTRA)
    return boto3.client("s3", config=configuration)


def make_elastic(es_endpoint: str, **kwargs) -> Elasticsearch:
    session = boto3.session.Session()
    credentials = session.get_credentials().get_frozen_credentials()
    awsauth = AWSRequestsAuth(
        # These environment variables are automatically set by Lambda
        aws_access_key=credentials.access_key,
        aws_secret_access_key=credentials.secret_key,
        aws_token=credentials.token,
        aws_host=urllib.parse.urlparse(es_endpoint).hostname,
        aws_region=session.region_name,
        aws_service="es",
    )

    return elasticsearch.Elasticsearch(
        hosts=es_endpoint,
        http_auth=awsauth,
        verify_certs=True,
        connection_class=elasticsearch.RequestsHttpConnection,
        **kwargs,
    )


def get_es_aliases(es) -> frozenset[str]:
    return frozenset(a for v in es.indices.get_alias().values() for a in v["aliases"])


def normalize_object_version_id(version_id: str | None) -> str:
    # Ensure the same versionId list-object-versions in the enterprise bulk_scanner.
    # We get version ID as empty string for unversioned objects when event comes
    # from EventBridge CloudTrail hack, so it also should be normalized to "null".
    return version_id or "null"


def get_object_doc_id(key: str, version_id: str | None) -> str:
    """
    Generate unique value for every object in the bucket to be used as
    document `_id`. This value must not exceed 512 bytes in size:
    https://www.elastic.co/guide/en/elasticsearch/reference/7.10/mapping-id-field.html.
    # TODO: both object key and version ID are up to 1024 bytes long, so
    # we need to use something like `_hash(key) + _hash(version_id)` to
    # overcome the mentioned size restriction.
    """
    return f"{key}:{normalize_object_version_id(version_id)}"


def get_manifest_doc_id(manifest_hash: str) -> str:
    assert len(manifest_hash) <= 512, "Manifest hash must not exceed 512 characters"
    return f"mnfst:{manifest_hash}"


def get_manifest_entry_doc_id(manifest_hash: str, logical_key: str) -> str:
    _id = f"entry:{manifest_hash}:{_hash_string(logical_key)}"
    assert len(_id) <= 512, "Manifest entry document ID must not exceed 512 characters"
    return _id


def get_ptr_doc_id(pkg_name: str, tag: str) -> str:
    _id = f"ptr:{_hash_string(pkg_name + '/' + tag)}"
    assert len(_id) <= 512, "Pointer document ID must not exceed 512 characters"
    return _id


def _hash_string(s: str) -> str:
    return hashlib.sha256(s.encode()).hexdigest()
