# Chunked Checksums

This variant of sha2-256 is designed to enable large files
to be efficiently uploaded and hashed in parallel using uniform size chunks
(8 MiB to start with), with the final result being a "top hash"
that doesn't depend on the upload order.

The algorithm has an upper limit of 10,000 chunks.
If the file is larger than 80,000 MiB,
it will double the chunk size until the number of chunks
is under that limit.

If the file is zero bytes, the top hash is the sha2-256 hash of the empty string.

## Inspiration

This algorithm is inspired by Amazon's
[S3 Checksums](https://aws.amazon.com/blogs/aws/new-additional-checksum-algorithms-for-amazon-s3/)
implementation.

The key differences are:

* Fixing the chunk size as (starting from) 8 MiB.
* Always hashing the result (even if source_size < 8 MiB).

It can reuse hashes generated by
[create_multipart_upload](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/create_multipart_upload.html)
as of at least [boto3 1.34.44](https://pypi.org/project/boto3/1.34.44/) 
(2024-02-16), simply by rehashing the value if source_size < 8 MiB.

## Multiformats Registration

* Name: sha2-256-chunked
* Prefix: 0xb510
* Status: draft
* Type: multihash
* Description: Hash of concatenated SHA2-256 digests of 8*2^n MiB source chunks;
  n = ceil(log2(source_size/(10^4 * 8MiB)))
* Registrar: [multiformats/multicodec](https://github.com/multiformats/multicodec) 
* Registration Date: [2024-02-23](https://github.com/multiformats/multicodec/pull/343)