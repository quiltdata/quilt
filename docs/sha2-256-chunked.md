# sha2-256-chunked

## DRAFT multihash codec 0xb510

> Hash of concatenated SHA2-256 digests of 8*2^n MiB source chunks
> where n = log(source_size / (10^4 * 8MiB))

This variant of sha2-256 is designed to enable very large files
to be efficiently uploaded and hashed in parallel using fixed size chunks
(8MiB to start with), with the final result being a "top hash"
that doesn't depend on the upload order.

The algorithm has an upper limit of 10,000 hashes.
If the file is larger than 80,000 MiB,
it will double the block size until the number of blocks
is under that limit.

```pseudocode
n = floor(log(source_size / (10_000 * 8 MiB)))
chunk_size = 8*2^n MiB
```

## Inspiration

This algorithm is inspired by Amazon's 
[S3 Checksums](https://aws.amazon.com/blogs/aws/new-additional-checksum-algorithms-for-amazon-s3/)
implementation.

The key differences are:

* Fixing the chunk size as (starting form) 8MiB.
* Always hashing the result (even if source_size < 8MiB).

It can reuse hashes generated by
[create_multipart_upload](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/create_multipart_upload.html)
as of at least [boto3 1.34.44](https://pypi.org/project/boto3/1.34.44/) 
(2024-02-16), simply by rehashing the value if source_size < 8MiB.

## Status 2024-02-21

It has been submitted for draft
[multiformats registration](https://github.com/multiformats/multiformats/blob/master/contributing.md#multiformats-registrations)
under the name `sha2-256-chunked` using the prefix `0xb510`.
