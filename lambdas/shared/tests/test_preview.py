"""
Preview helper functions
"""
import os
import pathlib
from unittest import TestCase
from unittest.mock import patch

import pyarrow.parquet as pq
from py_w3c.validators.html.validator import HTMLValidator

from t4_lambda_shared.preview import (
    extract_excel,
    extract_fcs,
    extract_parquet,
    get_bytes,
    get_preview_lines,
)

TEST_EXTRACT_PARQUET_MAX_BYTES = 10_000
BASE_DIR = pathlib.Path(__file__).parent / 'data'
ACCEPTABLE_ERROR_MESSAGES = [
    'Start tag seen without seeing a doctype first. Expected ‚Äú<!DOCTYPE html>‚Äù.',
    'Element ‚Äúhead‚Äù is missing a required instance of child element ‚Äútitle‚Äù.',
    'Element ‚Äústyle‚Äù not allowed as child of element ‚Äúdiv‚Äù in this context. '
    '(Suppressing further errors from this subtree.)',
    'The ‚Äúborder‚Äù attribute on the ‚Äútable‚Äù element is obsolete. Use CSS instead.',
]


def iterate_chunks(file_obj, chunk_size=4096):
    return iter(lambda: file_obj.read(chunk_size), b'')


class TestPreview(TestCase):
    """Tests the preview functions"""
    # 15_000 is magic = exact number of cells (cols*rows) in test file
    def test_extract_parquet(self):
        file = BASE_DIR / 'amazon-reviews-1000.snappy.parquet'
        cell_value = '<td>TSD Airsoft/Paintball Full-Face Mask, Goggle Lens</td>'

        with patch('t4_lambda_shared.preview.get_available_memory') as mem_mock:
            mem_mock.return_value = 1
            with open(file, mode='rb') as parquet:
                body, info = extract_parquet(parquet, max_bytes=TEST_EXTRACT_PARQUET_MAX_BYTES)
                assert all(bracket in body for bracket in ('<', '>'))
                assert body.count('<') == body.count('>'), \
                    'expected matching HTML tags'
                assert cell_value not in body, 'only expected columns'
                assert 'skipped rows' in info['warnings']

        with open(file, mode='rb') as parquet:
            body, info = extract_parquet(parquet, as_html=True, max_bytes=TEST_EXTRACT_PARQUET_MAX_BYTES)
            assert cell_value in body, 'missing expected HTML cell'

        with open(file, mode='rb') as parquet:
            body, info = extract_parquet(parquet, skip_rows=True, max_bytes=TEST_EXTRACT_PARQUET_MAX_BYTES)
            assert 'skipped rows' in info['warnings']
            assert cell_value not in body, 'only expected columns'

        with open(file, mode='rb') as parquet:
            body, info = extract_parquet(parquet, as_html=False, max_bytes=TEST_EXTRACT_PARQUET_MAX_BYTES)
            assert all(bracket not in body for bracket in ('<', '>')), \
                'did not expect HTML'
            parquet_file = pq.ParquetFile(file)
            assert all(
                column in info['schema']['names']
                for column in parquet_file.schema.names
            )
            assert [
                parquet_file.metadata.num_rows, parquet_file.metadata.num_columns
            ] == info['shape'], 'Unexpected number of rows or columns'

    def test_excel(self):
        """test XLS, XLSX parsing"""
        test_files = {
            "Revised.Haplogroups.1000G.20140205.xlsx": {
                "contents": [
                    "Continent",
                    "Population",
                    "ID",
                    "Macrohaplogroup",
                    "Haplogroup",
                    "Informative SNPs",
                    "NA19239",
                    "NA19256",
                    "E1b1a1a1g1a2",
                ]
            },
            "lclarke_phase1_sequence_stats_20120330.xls": {
                "contents": [
                    "Breakdown of data generated by project, technology, submitting centre",
                    "92219554043",
                    "90363687334"
                ]
            }
        }
        vld = HTMLValidator()
        for file, expected_data in test_files.items():
            in_file = os.path.join(BASE_DIR, "excel", file)
            with open(in_file, mode="rb") as excel:
                for html in [False, True]:
                    body, _ = extract_excel(excel, as_html=html)
                    # print(body)
                    tags = ['<div>', '<tbody>', '<th>', '<td>', '<tr>']
                    if html:
                        vld.validate_fragment(body)
                        assert all(t in body for t in tags)
                        serious_errors = [
                            e for e in vld.errors
                            if e["message"] not in ACCEPTABLE_ERROR_MESSAGES
                        ]
                        assert not serious_errors
                        print(vld.warnings)
                    else:
                        assert not any(t in body for t in tags)
                    assert all(c in body for c in expected_data["contents"])

    def test_fcs(self):
        """test FCS parsing"""
        # store test files and expectations
        test_files = {
            'normal.fcs': {
                'columns_string': 'FSC-A,SSC-A,FL1-A,FL2-A,FL3-A,FL4-A,FSC-H,SSC-H,FL1-H,FL2-H,FL3-H,FL4-H,Width,Time',
                'in_body': '<th>FL3-H</th>',
                'in_meta_keys': '#P1MaxUsefulDataChannel',
                'in_meta_values': '491519',
                'has_warnings': False,
            },
            'meta_only.fcs': {
                'in_meta_keys': '_channel_names_',
                'in_meta_values': 'Compensation Controls_G710 Stained Control.fcs',
                'has_warnings': True,
            },
        }
        for file, expected_data in test_files.items():
            in_file = os.path.join(BASE_DIR, 'fcs', file)

            with open(in_file, mode='rb') as fcs:
                body, info = extract_fcs(fcs)
                if body != "":
                    assert expected_data['in_body'] in body
                    assert not expected_data.get('has_warnings')
                else:
                    assert expected_data['has_warnings']
                    assert info['warnings']
                assert expected_data['in_meta_keys'] in info['metadata'].keys()
                assert expected_data['in_meta_values'] in info['metadata'].values()
                # when there's a body, check if columns only works
                if expected_data.get('in_body'):
                    # move to start so we can use the file-like a second time
                    fcs.seek(0)
                    body, info = extract_fcs(fcs, as_html=False)
                    assert body == expected_data['columns_string']

    def test_long(self):
        """test a text file with lots of lines"""
        txt = BASE_DIR / 'long.txt'
        max_lines = 500
        max_bytes = 10000
        with open(txt, 'rb') as file_obj:
            lines = get_preview_lines(iterate_chunks(file_obj), None, max_lines, max_bytes)

        assert len(lines) == max_lines, 'unexpected number of lines'
        assert lines[0] == 'Line 1', 'unexpected first line'
        assert lines[-1] == f'Line {max_lines}', 'unexpected last line'

    def test_long_gz(self):
        """test a gzipped text file with lots of lines"""
        txt = BASE_DIR / 'long.txt.gz'
        max_lines = 500
        max_bytes = 10000
        with open(txt, 'rb') as file_obj:
            lines = get_preview_lines(iterate_chunks(file_obj), 'gz', max_lines, max_bytes)

        assert len(lines) == max_lines, 'unexpected number of lines'
        assert lines[0] == 'Line 1', 'unexpected first line'
        assert lines[-1] == f'Line {max_lines}', 'unexpected last line'

    def test_txt_max_bytes(self):
        """test truncation to CATALOG_LIMIT_BYTES"""
        txt = BASE_DIR / 'two-line.txt'
        max_lines = 500
        max_bytes = 5
        with open(txt, 'rb') as file_obj:
            lines = get_preview_lines(iterate_chunks(file_obj), None, max_lines, max_bytes)
        assert len(lines) == 1, 'failed to truncate bytes'
        assert lines[0] == '1234üòä', 'failed to truncate bytes'

    def test_txt_max_bytes_one_line(self):
        """test truncation to CATALOG_LIMIT_BYTES"""
        txt = BASE_DIR / 'one-line.txt'
        max_lines = 500
        max_bytes = 8
        chunk_size = 10
        with open(txt, 'rb') as file_obj:
            lines = get_preview_lines(
                iterate_chunks(file_obj, chunk_size),
                None,
                max_lines,
                max_bytes
            )
        assert len(lines) == 1, 'failed to truncate bytes'
        assert lines[0] == 'üö∑üöØ', 'failed to truncate bytes'

    def test_bytes(self):
        txt = BASE_DIR / 'long.txt.gz'
        with open(txt, 'rb') as file_obj:
            buffer = get_bytes(file_obj, 'gz')
        lines = buffer.getvalue().splitlines()
        assert lines[0] == b'Line 1'
        assert lines[-1] == b'Line 999'
